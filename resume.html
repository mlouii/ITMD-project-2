<!DOCTYPE html>
<html lang = "en">
<head>
	<meta charset = "UTF-8">
	<title> Mark Lou Resume </title>
</head>
<body>
	<header>
		<nav>
			<ul>
				<li> <a href = "index.html"> Homepage </a> </li>
				<li> <a href = "hamsters.html"> Cool Hampters for Hire </a> </li>
				<li> <a href = "resume.html"> My Resume </a> </li>
			</ul>
		</nav>
	</header>
	<h1> Mark Lou </h1>
    <a href="http://www.linkedin.com/in/mlouii"><p>Linkedin is here :)))))))))))))</p></a>

    <h2> Education </h2>
    <h3> Illinois Insitute of Technology - <em>MS in Data Science, BS in Computer Science</em></h3>
    <ul>
        <li><strong>Relevant Coursework</strong>Databases, Data Mining, Big Data Technologies, Operating Systems, Probability and Statistics, 
            Mathematical Statistics 
            </li>
        <li><strong>Current GPA:</strong>3.89</li>
        <li><strong>TA Positions:</strong> Data Structures and Algorithms, Intro to Profession (CS) </li>
    </ul>
    <h2> Experience </h2>
    <h3> Chan Zuckerberg Initiative – <em> Data Engineering Intern (Education)  </em></h3>
    <ul>
        <li> Spearheaded a project giving data scientists the ability to understand exactly how users spend their time on our platform</li>
        <li>Determined strategies for use of dbt, and incorporating it with our Airflow ETL pipelines into Snowflake</li>
        <li> Expedited development processes by transferring insights between front-end engineers and data scientists</li>

    </ul>
    <h3> Esri  – <em> Software Engineering Intern  </em></h3>
    <ul>
        <li> Worked with gRPC and Kubernetes to create pipelines for data into our real time data analytics service</li>
        <li>Expanded features of a Node.js API to allow users to transform and query geospatial data from Snowflake</li>
        <li> Added comprehensive logging functionality to a Python service and debugged our ArcGIS Python API</li>

    </ul>
    <h3> Caterpillar – <em> Software Engineering Intern (Architecture and Analytics)   </em></h3>
    <ul>
        <li> Used AWS Lambda and SQS to pipeline vast amounts of data between Snowflake, S3, and DynamoDB
        </li>
        <li>Reduced latency of our Python Snowflake integrations package by 50% through refactoring</li>
        <li> Orchestrated anomaly detection using SageMaker on custom trained machine learning models</li>

    </ul>
    <h3> ProvenAir – <em> Software Engineering Intern  </em></h3>
    <ul>
        <li> Automated a manual data ingestion workload by integrating an OCR pipeline, reducing time from days to minutes
        </li>
        <li>Greatly improved text recognition by denoising dirty scans with the SciKit-Image library</li>
        <li>Classified documents using NLP and XGBoost achieving 90% accuracy rate over a wide range of inputs</li>
    </ul>
    <h2> Skills </h2>
    <ul>
        <li> <strong>Language</strong> – Python, SQL, Typescript, HTML, CSS, Java, Ocaml
        </li>
        <li> 
           <strong>Frameworks and Tools</strong> - Airflow, dbt, AWS, Azure, Firebase, Snowflake, PostgreSQL, Flask, Angular</li>
    </ul>
</body>
</html>